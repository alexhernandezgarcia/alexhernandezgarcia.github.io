---
date: 2019-07-17
title: Global visual salience
layout: page
---
Our pre-print ["Global visual salience of competing stimuli"](https://doi.org/10.31234/osf.io/z7qp5) is out on PsyArXiv! We run an eye-tracking experiment showing natural images side by side and trained a machine learning model to predict to direction of the first fixation (left or right) given a pair of images. The coefficients learned for each image characterize the likelihood of each image to attract the first saccade, when shown next to another competing stimulus, which we called _global visual salience_.

Interestingly, the global visual salience is independent of the local salience maps of the images, that is, the saccadic choice was not driven by local salient properties, but rather by, in part, the semantic content of the images. For instance, we found that faces and images with people have a higher global salience than urban, indoor or natural scenes.

We also confirmed a relatively strong general preference for the left image, although with high variability across participants. Besides, we found no influence of other aspects such as the familiarity with one of the images or the task to determine which of the two images had been seen before.

{% include image.html url="/assets/images/global-salience-stimuli.png" description="Images used in the eye-tracking experiment, sorted by their global visual salience, from left to right, and top to bottom." %}
