---
layout: slides_warbler
title: IFT 6760B A25 - Review of relevant background 
---

name: review-20250908
class: title, middle

## Probabilistic inference with GFlowNets
### IFT 6760B A25

#### .gray224[September 8th - Session 2]
### .gray224[Review of relevant background]

.smaller[.footer[
Slides: [alexhernandezgarcia.github.io/teaching/mlprojects24/slides/{{ name }}](https://alexhernandezgarcia.github.io/teaching/gflownets25/slides/{{ name }})
]]

.center[
<a href="http://www.umontreal.ca/"><img src="../../../assets/images/slides/logos/udem-white.png" alt="UdeM" style="height: 6em"></a>
]

Alex Hernández-García (he/il/él)

.footer[[alexhernandezgarcia.github.io](https://alexhernandezgarcia.github.io/) | [alex.hernandez-garcia@mila.quebec](mailto:alex.hernandez-garcia@mila.quebec)] | [alexhergar.bsky.social](https://bsky.app/profile/alexhergar.bsky.social) [![:scale 1em](../../../assets/images/slides/misc/bluesky.png)](https://bsky.app/profile/alexhergar.bsky.social)<br>

---

## Objectives of this session

- Review the topics most relevant to introduce GFlowNets:
    - Generative machine learning
    - Sampling methods
    - Reinforcement learning

--

The goal is that at the end of the session:
- You will have a fresh overview of topics that will come up during the rest of the course.
- You will be able to identify the topics you may want to check or review.
- You will have a access to a set of resources to review this literature.

???

Obviously, all these topics will be covered in a very superficial way.

---

count: false

## Outline

- [Generative machine learning](#generativeml)
- [Sampling methods](#sampling)
- [Reinforcement learning](#reinforcementlearning)

---

count: false
name: generativeml
class: title, middle

## Generative machine learning
### A brief overview

.center[![:scale 30%](../../../assets/images/teaching/gflownets/misc/fractal.jpg)]

---

## Generative models

A generative model aims to learn the probability distribution

$$
p_{\theta}(x), x \in \mathcal{X}
$$

where $x$ are the generated data points in the sample space $\mathcal{X}$, and $\theta$ are the (learnable) parameters of the generative model.

--

This is in contrast with typical supervised, _predictive_ models, which can be seen to learn

$$
p(y|x), x \in \mathcal{X}, y \in \mathcal{Y}
$$

where $y$ are annotations we would like to learn via classification or regression.

---

## Generative models

.references[
Foster. Generative deep learning. O'Reilly Media, Inc., 2022.
]

The typical _generative modelling framework_ is as follows:

- We assume we have a data set of observations $x_1, x_2, \ldots, x_N$.
- We assume these observations have been generated according to an unknown distribution $p_{data}(x)$.
- We set the goal of training a generative model $p_{\theta}(x)$ to mimic the data-generating distribution.
- We see a trained model as successful if:
    - The generated examples appear to have been drawn from $p_{data}(x)$.
    - But they are suitably different from the training data.

---

## Generative models
### Maximum-likelihood estimation

A very common technique to determine the parameters of machine learning models is _maximum likelihood estimation_ (MLE):

- The _likelihood_ of a set of parameters $\theta$ is a function that measures the plausibility of the parameters given a data point $x$: $L(\theta | x) = p_{\theta}(x)$
- Assuming that the observations in the data set $\mathcal{D}$ are independent and identically distributed (i.i.d), then $L(\theta | x) = \prod p_{\theta}(x)$
- In practice, it is better to maximise the log-likelihood:

$$\theta^{\star} = \text{arg max} \sum \log p_{\theta}(x)$$

<br>
.conclusion[MLE or variations of it are at the core of many generative modelling algorithms.]

---

## Markov chains


---

name: title
class: title, middle
count: false

## Probabilistic inference with GFlowNets
### IFT 6760B A25

#### .gray224[September 8th - Session 2]
### .gray224[Review of relevant background]

.bigger[.bigger[.highlight1[Break!]]]

.center[
<a href="http://www.umontreal.ca/"><img src="../../../assets/images/slides/logos/udem-white.png" alt="Mila" style="height: 6em"></a>
]

Alex Hernández-García (he/il/él)

.footer[[alexhernandezgarcia.github.io](https://alexhernandezgarcia.github.io/) | [alex.hernandez-garcia@mila.quebec](mailto:alex.hernandez-garcia@mila.quebec)] | [alexhergar.bsky.social](https://bsky.app/profile/alexhergar.bsky.social) [![:scale 1em](../../../assets/images/slides/misc/bluesky.png)](https://bsky.app/profile/alexhergar.bsky.social)<br>

---
name: title
class: title, middle
count: false

## Probabilistic inference with GFlowNets
### IFT 6760B A25

#### .gray224[September 8th - Session 2]
### .gray224[Review of relevant background]

.bigger[.bigger[.highlight1[Questions?]]]

.center[
<a href="http://www.umontreal.ca/"><img src="../../../assets/images/slides/logos/udem-white.png" alt="Mila" style="height: 6em"></a>
]

Alex Hernández-García (he/il/él)

.footer[[alexhernandezgarcia.github.io](https://alexhernandezgarcia.github.io/) | [alex.hernandez-garcia@mila.quebec](mailto:alex.hernandez-garcia@mila.quebec)] | [alexhergar.bsky.social](https://bsky.app/profile/alexhergar.bsky.social) [![:scale 1em](../../../assets/images/slides/misc/bluesky.png)](https://bsky.app/profile/alexhergar.bsky.social)<br>

